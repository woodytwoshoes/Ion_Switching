{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It occurred to me that sklearn's logistic regression is not really appropriate here. It's one-vs-all algorithm does not capture, I think, the correct loss penalty. All classes are not of the same type. The classes are actually linear in scale. I need a model which penalises predictions which are far away from actual values for open channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "plt.rcParams['agg.path.chunksize'] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "df_terminus = df.index.values[-1]\n",
    "batches = int(df_terminus/50)\n",
    "seconds_in_batch = 50\n",
    "freq = 10000\n",
    "\n",
    "print(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n"
     ]
    }
   ],
   "source": [
    "base = np.array([])\n",
    "for batch in range(batches):\n",
    "    mini_array = np.ones(50*10000)*batch\n",
    "    base = np.append(base,mini_array)\n",
    "print(len(base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['batch'] = base\n",
    "df['batch'] = df['batch'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new batch:  0\n",
      "25.0\n",
      "50.0\n",
      "End batch:  0  with padded fraction =  0.279776\n"
     ]
    }
   ],
   "source": [
    "batches = int(500/50)\n",
    "seconds_in_batch = 50\n",
    "freq = 10000\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for batch_number in range(batches):\n",
    "    print('Starting new batch: ', batch_number)\n",
    "    window_size = 0.001\n",
    "    batch_df = df[df.batch == batch_number]\n",
    "    indices = batch_df.index.values\n",
    "    padd_no = 0\n",
    "    for index in indices:\n",
    "        \n",
    "        if index % 25 == 0:\n",
    "            print(index)\n",
    "            \n",
    "        window_df = batch_df.loc[index-window_size/2:index+window_size/2]\n",
    "        open_channels = window_df.loc[index].open_channels\n",
    "        features = window_df.signal.values\n",
    "\n",
    "        if len(features) <= int(window_size*10000):\n",
    "\n",
    "            padding = np.ones((int(window_size*10000)-len(features))+1)*features.mean()\n",
    "            features = np.append(features,padding)\n",
    "            padd_no += 1\n",
    "            \n",
    "        X.append(features)\n",
    "        y.append(open_channels)\n",
    "    print('End batch: ', batch_number, ' with padded fraction = ', padd_no/len(indices))\n",
    "    \n",
    "    break\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 11)\n",
      "(500000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.20, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clf = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preds_val = clf.predict(X_val)\n",
    "\n",
    "preds_val = np.rint(preds_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print((preds_val == y_val).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is even better than when I used logistic regression (as expected). I can even 'clean up' the preds a little bit to make sure they're all reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preds_copy = preds_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(len(preds_val)):\n",
    "    if preds_val[i]<0:\n",
    "        preds_val[i] = 0\n",
    "print((preds_val == y_val).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'cleaning up' gives almost no improvement. It looks like almost all the values are within reasonable limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('f1 score for vals is ', f1_score(y_val, preds_val, average='macro'))\n",
    "\n",
    "preds_test = clf.predict(X_test)\n",
    "preds_test = np.rint(preds_test)\n",
    "print('f1 score for test is ', f1_score(y_test, preds_test, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above approach is more successful than log reg too. I'm also seeing similar F1 values for val and pred (val slightly higher) which is consistent with ML principles and suggests that I'm not overtraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print((preds_test == y_test).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the first time, let's throw a neural network at it and see what happens.\n",
    "https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/301_regression.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden_1): Linear(in_features=11, out_features=50, bias=True)\n",
      "  (hidden_2): Linear(in_features=50, out_features=20, bias=True)\n",
      "  (predict): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden_1,n_hidden_2, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_1 = torch.nn.Linear(n_feature, n_hidden_1)# hidden layer\n",
    "        self.hidden_2 = torch.nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.predict = torch.nn.Linear(n_hidden_2, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden_1(x))# activation function for hidden layer\n",
    "        x = F.relu(self.hidden_2(x))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "\n",
    "net = Net(n_feature=11, n_hidden_1=50, n_hidden_2=20, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "plt.ion()   # something about plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce compute power, I'll only take the first 1000 values of the train series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(X_train[:1000])\n",
    "x = x.to(device='cuda')\n",
    "y = torch.tensor(y_train[:1000])\n",
    "y = y.to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (hidden_1): Linear(in_features=11, out_features=50, bias=True)\n",
       "  (hidden_2): Linear(in_features=50, out_features=20, bias=True)\n",
       "  (predict): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.042949117720127106\n",
      "0.035172492265701294\n",
      "0.03508143872022629\n",
      "0.035070765763521194\n",
      "0.03506045415997505\n",
      "0.03505045548081398\n",
      "0.03504081070423126\n",
      "0.03503149002790451\n",
      "0.03502243384718895\n",
      "0.03501362353563309\n",
      "0.035005081444978714\n",
      "0.03499665483832359\n",
      "0.034988366067409515\n",
      "0.03498020023107529\n",
      "0.03497215360403061\n",
      "0.03496414050459862\n",
      "0.03495592251420021\n",
      "0.034947093576192856\n",
      "0.03493800386786461\n",
      "0.03492914140224457\n",
      "0.03492065519094467\n",
      "0.03491266444325447\n",
      "0.03490583971142769\n",
      "0.03489989787340164\n",
      "0.03489459678530693\n",
      "0.03488977625966072\n",
      "0.0348852276802063\n",
      "0.03488088399171829\n",
      "0.034876711666584015\n",
      "0.03487265855073929\n",
      "0.03486871346831322\n",
      "0.034864895045757294\n",
      "0.03486116975545883\n",
      "0.03485756367444992\n",
      "0.03485405817627907\n",
      "0.034850653260946274\n",
      "0.03484736755490303\n",
      "0.03484416380524635\n",
      "0.034841034561395645\n",
      "0.034837983548641205\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for t in range(200):\n",
    "    \n",
    "    \n",
    "    prediction = net(x.float())     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y.float())     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "\n",
    "    if t % 5 == 0:\n",
    "        losses.append(loss)\n",
    "        print(loss.item())\n",
    "    # Sanity check\n",
    "    \n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_clean = prediction.detach().cpu().clone().numpy()\n",
    "\n",
    "y_clean = y.detach().cpu().clone().numpy()\n",
    "\n",
    "preds_clean = preds_clean.astype('float').rint()\n",
    "\n",
    "preds_clean = np.concatenate(preds_clean)\n",
    "\n",
    "preds_clean = np.rint(preds_clean)\n",
    "\n",
    "(preds_clean == y_clean).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be working. So what I'll do now is test the first 10000 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden_1): Linear(in_features=11, out_features=50, bias=True)\n",
      "  (hidden_2): Linear(in_features=50, out_features=20, bias=True)\n",
      "  (predict): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "0.051225725561380386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([10000])) that is different to the input size (torch.Size([10000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03650752082467079\n",
      "0.031662411987781525\n",
      "0.031633276492357254\n",
      "0.031632889062166214\n",
      "0.0316329263150692\n",
      "0.03163289651274681\n",
      "0.03163288161158562\n",
      "0.03163295239210129\n",
      "0.0316329263150692\n",
      "0.031632911413908005\n",
      "0.03163289651274681\n",
      "0.03163287416100502\n",
      "0.03163284435868263\n",
      "0.03163281828165054\n",
      "0.03163280338048935\n",
      "0.03163277357816696\n",
      "0.031632788479328156\n",
      "0.03163275867700577\n",
      "0.03163273632526398\n",
      "0.031632713973522186\n",
      "0.03163269907236099\n",
      "0.0316326841711998\n",
      "0.031632669270038605\n",
      "0.03163263946771622\n",
      "0.031632617115974426\n",
      "0.03163260594010353\n",
      "0.03163258731365204\n",
      "0.03163289651274681\n",
      "0.03163288161158562\n",
      "0.031632859259843826\n",
      "0.03163284435868263\n",
      "0.03163281828165054\n",
      "0.031632810831069946\n",
      "0.03163279592990875\n",
      "0.03163278102874756\n",
      "0.031632762402296066\n",
      "0.03163273632526398\n",
      "0.031632713973522186\n",
      "0.03163270279765129\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(X_train[:10000])\n",
    "x = x.to(device='cuda')\n",
    "y = torch.tensor(y_train[:10000])\n",
    "y = y.to(device='cuda')\n",
    "net = Net(n_feature=11, n_hidden_1=50, n_hidden_2=20, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "plt.ion()   # something about plotting\n",
    "\n",
    "net.cuda()\n",
    "\n",
    "losses = []\n",
    "for t in range(200):\n",
    "    \n",
    "    \n",
    "    prediction = net(x.float())     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y.float())     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "\n",
    "    if t % 5 == 0:\n",
    "        losses.append(loss)\n",
    "        print(loss.item())\n",
    "    \n",
    "    # Sanity check\n",
    "    \n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9673"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_clean = prediction.detach().cpu().clone().numpy()\n",
    "\n",
    "y_clean = y.detach().cpu().clone().numpy()\n",
    "\n",
    "preds_clean = np.concatenate(preds_clean)\n",
    "preds_clean = np.rint(preds_clean)\n",
    "\n",
    "(preds_clean == y_clean).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to use one of the old metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score for trains is  0.4916891170639964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('f1 score for trains is ', f1_score(y_clean, preds_clean, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'not actually a bad f1, although we definitely have to consider this may be overfitting. Let's see what kind of preds I get on the test set with this net as is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score for trains is  0.4923342471316885\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(X_val[:10000])\n",
    "x = x.to(device='cuda')\n",
    "y = torch.tensor(y_val[:10000])\n",
    "y = y.to(device='cuda')\n",
    "\n",
    "prediction = net(x.float())\n",
    "\n",
    "preds_clean = prediction.detach().cpu().clone().numpy()\n",
    "\n",
    "y_clean = y.detach().cpu().clone().numpy()\n",
    "\n",
    "preds_clean = np.concatenate(preds_clean)\n",
    "preds_clean = np.rint(preds_clean)\n",
    "\n",
    "(preds_clean == y_clean).mean()\n",
    "\n",
    "print('f1 score for vals is ', f1_score(y_clean, preds_clean, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 score for trains is  0.4923342471316885. That's actually not bad at all! Obviously this neural network isn't better than linear regression for this batch, but it's pretty good!. Similar f1 scores on train and val also indicates that I'm not overtraining. According to Andrew Ng it may also indicate that my neural net is not complicated enough... yet. I wonder if including more layers will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden_1): Linear(in_features=11, out_features=100, bias=True)\n",
      "  (hidden_2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (hidden_3): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (predict): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden_1,n_hidden_2, n_hidden_3, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_1 = torch.nn.Linear(n_feature, n_hidden_1)# hidden layer\n",
    "        self.hidden_2 = torch.nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.hidden_3 = torch.nn.Linear(n_hidden_2, n_hidden_3)\n",
    "        self.predict = torch.nn.Linear(n_hidden_3, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden_1(x))# activation function for hidden layer\n",
    "        x = F.relu(self.hidden_2(x))\n",
    "        x = F.relu(self.hidden_3(x))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "\n",
    "net = Net(n_feature=11, n_hidden_1=100, n_hidden_2=50, n_hidden_3=10, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "plt.ion()   # something about plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden_1): Linear(in_features=11, out_features=100, bias=True)\n",
      "  (hidden_2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (hidden_3): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (predict): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "0.031659338623285294\n",
      "0.0316479317843914\n",
      "0.03164779394865036\n",
      "0.031647659838199615\n",
      "0.03164752572774887\n",
      "0.03164739906787872\n",
      "0.03164726868271828\n",
      "0.03164714574813843\n",
      "0.03164701908826828\n",
      "0.031646888703107834\n",
      "0.031646765768527985\n",
      "0.031646646559238434\n",
      "0.031646523624658585\n",
      "0.031646400690078735\n",
      "0.03164628520607948\n",
      "0.031646162271499634\n",
      "0.03164605051279068\n",
      "0.03164593130350113\n",
      "0.03164581581950188\n",
      "0.03164571151137352\n",
      "0.03164559602737427\n",
      "0.031645480543375015\n",
      "0.03164537623524666\n",
      "0.031645264476537704\n",
      "0.03164515644311905\n",
      "0.031645048409700394\n",
      "0.03164494410157204\n",
      "0.03164483606815338\n",
      "0.03164473548531532\n",
      "0.031644634902477264\n",
      "0.031644534319639206\n",
      "0.03164443373680115\n",
      "0.03164433315396309\n",
      "0.03164423257112503\n",
      "0.03164413571357727\n",
      "0.03164403885602951\n",
      "0.03164393827319145\n",
      "0.03164384514093399\n",
      "0.03164375573396683\n",
      "0.031643662601709366\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(X_train[:10000])\n",
    "x = x.to(device='cuda')\n",
    "y = torch.tensor(y_train[:10000])\n",
    "y = y.to(device='cuda')\n",
    "\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "plt.ion()   # something about plotting\n",
    "\n",
    "net.cuda()\n",
    "\n",
    "losses = []\n",
    "for t in range(200):\n",
    "    \n",
    "    \n",
    "    prediction = net(x.float())     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y.float())     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "\n",
    "    if t % 5 == 0:\n",
    "        losses.append(loss)\n",
    "        print(loss.item())\n",
    "    \n",
    "    # Sanity check\n",
    "    \n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score for vals is  0.4923342471316885\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(X_val[:10000])\n",
    "x = x.to(device='cuda')\n",
    "y = torch.tensor(y_val[:10000])\n",
    "y = y.to(device='cuda')\n",
    "\n",
    "prediction = net(x.float())\n",
    "\n",
    "preds_clean = prediction.detach().cpu().clone().numpy()\n",
    "\n",
    "y_clean = y.detach().cpu().clone().numpy()\n",
    "\n",
    "preds_clean = np.concatenate(preds_clean)\n",
    "preds_clean = np.rint(preds_clean)\n",
    "\n",
    "(preds_clean == y_clean).mean()\n",
    "\n",
    "print('f1 score for vals is ', f1_score(y_clean, preds_clean, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 score for vals is  0.4923342471316885. So there's only a tiny improvement in the f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds_clean == 0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bugger. The neural net is predicting 0 every time! There aren't enough open_channel values of 1 or more to knock the NN in that direction. This is really bad. I wonder if there's a way to weight the classes. First I'll train on the whole first batch to make sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden_1): Linear(in_features=11, out_features=100, bias=True)\n",
      "  (hidden_2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (hidden_3): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (predict): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "net = Net(n_feature=11, n_hidden_1=100, n_hidden_2=50, n_hidden_3=10, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "plt.ion()   # something about plotting\n",
    "\n",
    "x = torch.tensor(X_train)\n",
    "y = torch.tensor(y_train)\n",
    "\n",
    "net.cuda()\n",
    "\n",
    "train_dataset = data.TensorDataset(x, y)\n",
    "train_loader = data.DataLoader(dataset, batch_size=1000,pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.031001614406704903\n",
      "0.030988197773694992\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for t in range(10):\n",
    "    \n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # the dataset \"lives\" in the CPU, so do our mini-batches\n",
    "        # therefore, we need to send those mini-batches to the\n",
    "        # device where the model \"lives\"\n",
    "        x_batch = x_batch.to('cuda')\n",
    "        y_batch = y_batch.to('cuda')\n",
    "        \n",
    "        prediction = net(x_batch.float())     # input x and predict based on x\n",
    "\n",
    "        loss = loss_func(prediction, y_batch.float())     # must be (1. nn output, 2. target)\n",
    "\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "\n",
    "    if t % 5 == 0:\n",
    "        losses.append(loss)\n",
    "        print(loss.item())\n",
    "    \n",
    "    # Sanity check\n",
    "    \n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0310, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " tensor(0.0310, device='cuda:0', grad_fn=<MseLossBackward>)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "f1 score for vals is  0.4920104532960055\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.tensor(X_val)\n",
    "x = x.to(device='cuda')\n",
    "y = torch.tensor(y_val)\n",
    "y = y.to(device='cuda')\n",
    "\n",
    "prediction = net(x.float())\n",
    "\n",
    "preds_clean = prediction.detach().cpu().clone().numpy()\n",
    "\n",
    "y_clean = y.detach().cpu().clone().numpy()\n",
    "\n",
    "preds_clean = np.concatenate(preds_clean)\n",
    "preds_clean = np.rint(preds_clean)\n",
    "\n",
    "print((preds_clean != 0).mean())\n",
    "\n",
    "print('f1 score for vals is ', f1_score(y_clean, preds_clean, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural net is still predicting 0 ever time. This is really no good. The more I look into this, the more I think that I need to 'weight' the predictions accordingly. Alternatively, I could use the Method 2: Ordinal target function to emphasize that the open channels targets is an ordinal classifier. https://stats.stackexchange.com/questions/222073/classification-with-ordered-classes. \n",
    "\n",
    "For now I'll dive into trying to 'weight' the classes appropriately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden_1): Linear(in_features=11, out_features=100, bias=True)\n",
      "  (hidden_2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (hidden_3): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (predict): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net(n_feature=11, n_hidden_1=100, n_hidden_2=50, n_hidden_3=10, n_output=1)     # define the network\n",
    "net.cuda()\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "plt.ion()   # something about plotting\n",
    "\n",
    "x = torch.tensor(X_train)\n",
    "y = y_train\n",
    "\n",
    "class_sample_count = np.array(\n",
    "    [len(np.where(y == t)[0]) for t in np.unique(y)])\n",
    "\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[int(t)] for t in y])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "samples_weigth = samples_weight.double()\n",
    "sampler = data.sampler.WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "y = torch.tensor(y_train)\n",
    "\n",
    "train_dataset = data.TensorDataset(x, y)\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(dataset, batch_size=1000, sampler=sampler, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24984954297542572\n",
      "0.250304639339447\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for t in range(10):\n",
    "    \n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # the dataset \"lives\" in the CPU, so do our mini-batches\n",
    "        # therefore, we need to send those mini-batches to the\n",
    "        # device where the model \"lives\"\n",
    "        x_batch = x_batch.to('cuda')\n",
    "        y_batch = y_batch.to('cuda')\n",
    "        \n",
    "        prediction = net(x_batch.float())     # input x and predict based on x\n",
    "\n",
    "        loss = loss_func(prediction, y_batch.float())     # must be (1. nn output, 2. target)\n",
    "\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "\n",
    "    if t % 5 == 0:\n",
    "        losses.append(loss)\n",
    "        print(loss.item())\n",
    "    \n",
    "    # Sanity check\n",
    "    \n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, even though the classes are weighted, the losses are MUCH MUCH higher. Let's see if our f1 score improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999666666666667\n",
      "f1 score for vals is  0.030487190106642203\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.tensor(X_val)\n",
    "x = x.to(device='cuda')\n",
    "y = torch.tensor(y_val)\n",
    "y = y.to(device='cuda')\n",
    "\n",
    "prediction = net(x.float())\n",
    "\n",
    "preds_clean = prediction.detach().cpu().clone().numpy()\n",
    "\n",
    "y_clean = y.detach().cpu().clone().numpy()\n",
    "\n",
    "preds_clean = np.concatenate(preds_clean)\n",
    "preds_clean = np.rint(preds_clean)\n",
    "\n",
    "print((preds_clean != 0).mean())\n",
    "\n",
    "print('f1 score for vals is ', f1_score(y_clean, preds_clean, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now the model almost NEVER predicts 0 because it's so underweighted. In addition, our f1 score is almost 0. So weighting the classes in this way has actually hurt out results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9685444444444444"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_val==0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.9685444444444444 is the fraction of the validation set that has 0 channels open. This class imbalance is massively throwing the loss function off. I think in order to explore this a bit more I'm going to go back and use more basic models in sklearn to attempt different strategies to weighting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
